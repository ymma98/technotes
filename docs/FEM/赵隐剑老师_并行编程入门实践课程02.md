# 赵隐剑老师_并行编程入门实践课程 02

* 该笔记对应以下 b 站视频，主要讲 MPI 的使用
	*  [并行编程入门与实践4](https://www.bilibili.com/video/BV1PQU2YvE8J/?spm_id_from=333.788.recommend_more_video.4&vd_source=b7bbd99721bfe117cc47d14c9f45af86)

其它 MPI 教程:
	* [Paul Norvig's basic Guide](https://www.paulnorvig.com/guides/using-mpi-with-c.html) 
	* [MPI tutorial](https://mpitutorial.com/tutorials/) 本篇笔记主要参考的是这个

[TOC]


---

## MPI

MPI（Message Passing Interface），主要是处理进程之间消息通信。在 MPI 中，通常运行同一个程序的多个实例，每个实例是一个 MPI 进程。与线程不同的是，每个进程都有自己的独立内存空间。

通讯器（communicator）。通讯器定义了一组能够互相发消息的进程。在这组进程中，每个进程会被分配一个序号，称作**秩（rank）**，通常从 0 开始，到 `size-1`，其中 `size` 是该通信器中进程的总数。默认的通信器是 **MPI_COMM_WORLD**，它包含了所有的进程。

进程之间通过消息传递来交换数据。MPI 提供了多种消息传递函数，包括阻塞和非阻塞的、点对点和集合通信的消息传递方式。

通信的基础建立在不同进程间发送和接收操作。一个进程可以通过指定另一个进程的秩以及一个独一无二的消息标签（tag）来发送消息给另一个进程。接受者可以发送一个接收特定标签标记的消息的请求（或者也可以完全不管标签，接收任何消息），然后依次处理接收到的数据。类似这样的涉及一个发送者以及一个接受者的通信被称作点对点（point-to-point）通信。

当然在很多情况下，某个进程可能需要跟所有其他进程通信。比如主进程想发一个广播给所有的从进程。在这种情况下，手动去写一个个进程点对点的信息传递就显得很笨拙。而且事实上这样会导致网络利用率低下。MPI 有专门的接口来帮我们处理这类所有进程间的集体性（collective）通信。

编译:

```cpp
mpic++ main.cpp -o main
mpirun -np 4 ./main
```

### Initializing and Finalizing MPI

-   `MPI_Init`: Initializes the MPI environment. It must be called before any other MPI functions.
-   `MPI_Finalize`: Cleans up the MPI environment. After calling this, no more MPI functions can be used.

```cpp
#include <mpi.h>
#include <iostream>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);  // Initialize MPI
    
    // ... MPI code here ...

    MPI_Finalize();  // Finalize MPI
    return 0;
}
```


### Querying Process Information

-  `MPI_Comm_size`: Returns the total number of processes in a given communicator.
-   `MPI_Comm_rank`: Returns the rank of the current process within that communicator.
<!--stackedit_data:
eyJoaXN0b3J5IjpbNjM3NDY5MTY4LC0xNzgxMTc1LC0yMDczMz
A1NjEsLTMxNDQwMzYyMiwtNjQ4NzczOTAyLC03MDI4ODM4ODks
LTI4NjA4NjY3NCwtMTU1OTcwOTE0OCw3NjI0NDgyOTAsMTc3NT
IwODcxMl19
-->