# 赵隐剑老师_并行编程入门实践课程

* 该笔记对应 b 站视频
	*  [并行编程入门与实践1](https://www.bilibili.com/video/BV1dVSdYLE8R/?spm_id_from=333.999.0.0&vd_source=b7bbd99721bfe117cc47d14c9f45af86)
	* [并行编程入门与实践2](https://www.bilibili.com/video/BV1wNDRYdEHw/?spm_id_from=333.999.0.0&vd_source=b7bbd99721bfe117cc47d14c9f45af86)
	* [并行编程入门与实践3](https://www.bilibili.com/video/BV1hcm9YfEkr?spm_id_from=333.788.recommend_more_video.1&vd_source=b7bbd99721bfe117cc47d14c9f45af86)

* 其它 OpenMP 教程
	* [OpenMP101](https://github.com/ysh329/OpenMP-101?tab=readme-ov-file)
	* [OpenMP tutorial by Blaise Barney, Lawrence Livermore National Laboratory](https://hpc-tutorials.llnl.gov/openmp/)
	* [Guide into OpenMP](https://bisqwit.iki.fi/story/howto/openmp/)


课程信息: 

![输入图片说明](https://github.com/ymma98/picx-images-hosting/raw/master/20241210/image.1027v3stf1.webp){ width="400" }

* 记笔记需要的 chatgpt prompt
	* fortran (png) -> cpp

```bash
1. Fortran Code Recognition: Recognize the Fortran code shown in this Vim screenshot and convert it into text form.

2. Modern C++ Equivalent: Rewrite the Fortran code in modern C++ using best practices. Specifically:

- Use appropriate C++ constructs and libraries.

- Ensure the C++ code is idiomatic, clean, and adheres to current standards (e.g., C++17 or C++20).

- Provide comments to explain how the Fortran logic maps to the equivalent C++ constructs.

Please ensure the converted C++ code is practical, efficient, and easily understandable.
```

[TOC]


---

## 并行程序分类

* 共享内存式 (shared memory)
	* OpenMP
	* 多线程

![输入图片说明](https://github.com/ymma98/picx-images-hosting/raw/master/20241210/image.51e79irczs.webp){width="600"}

![输入图片说明](https://github.com/ymma98/picx-images-hosting/raw/master/20241210/image.231x60ftz2.webp){width="300"}




* 分配内存式 (distributed memory)
	* 理论上没有上限
	* cpu与自己节点的内存快速交互
	* 通过网络沟通各个节点，交互数据
	* 并行进程之间的信息交互需要人工编写


![输入图片说明](https://github.com/ymma98/picx-images-hosting/raw/master/20241210/image.1lbvhffowk.webp){width="300"}


## openmp 重要函数

* fortran 版本

```fortran
use omp_lib ! provides OpenMP functions in Fortran

call omp_set_num_threads() ! sets the number of OpenMP threads that will be used in the subsequent parallel regions
omp_get_thread_num() ! retrieves the unique ID of the calling thread (ranging from 0 to num_threads-1)

! `!$omp` 表示该行是 OpenMP 的并行编程指令，而不是普通注释 

! define a parallel region in Fortran. Code inside this region is executed by multiple threads
!$omp parallel  
!$omp end parallel

! sets a parallel region that specifically uses 3 threads
!$omp parallel num_threads(3)

! creates a parallel region where `a`, `b`, and `c` are private variables, meaning each thread gets its own copy
!$omp parallel private(a, b, c)

! `default(none)` 强制程序员显式地声明每个变量的共享属性, `p`, `q`, and `r` are shared by all threads
!$omp parallel default(none) shared(p, q, r)
!$omp parallel default(private) shared(a, b, c)
! `i` is private to each thread but initialized with the value of `i` from outside the parallel region.
!$omp parallel firstprivate(i)
```

* 对应c++
	-   `use omp_lib` -> `#include <omp.h>` in C++
	-   `call omp_set_num_threads(X)` -> `omp_set_num_threads(X);`
	-   `omp_get_thread_num()` -> `omp_get_thread_num();`
	-   `!$omp parallel` ... `!$omp end parallel` -> `#pragma omp parallel` ... (block of code) ... end block
	-   Clauses on `!$omp parallel` translate similarly to clauses on `#pragma omp parallel` in C++.


* example

```cpp
#include <iostream>
#include <omp.h>

int main() {
    // Setting the number of OpenMP threads: omp_set_num_threads(<number>);     
    omp_set_num_threads(4); 

    // Getting the thread number:
    int thread_num = omp_get_thread_num(); // Typically called inside a parallel region
    // Note: Outside a parallel region, usually omp_get_thread_num() returns 0.
    
    // define a parallel region 
    #pragma omp parallel
    {
        // Code inside this block runs in parallel
        int my_id = omp_get_thread_num();
        // ... do parallel work ...
    }

    // Equivalent to !$omp parallel num_threads(3):
    #pragma omp parallel num_threads(3)
    {
        int my_id = omp_get_thread_num();
        // This block runs in parallel with exactly 3 threads
    }

    // Equivalent to !$omp parallel private(a, b, c):
    int a = 0, b = 1, c = 2; 
    #pragma omp parallel private(a, b, c)
    {
        // Each thread has its own a, b, c
        // The values of a, b, c are uninitialized or unspecified in this new scope,
        // For clarity, re-declare inside the block if needed:
        a = 10; b = 20; c = 30; 
    }

    // Equivalent to !$omp parallel default(none) shared(p, q, r):
    int p = 100, q = 200, r = 300;
    #pragma omp parallel default(none) shared(p, q, r)
    {
        // p, q, r are shared among threads.
        // Any other variable used in this scope must be explicitly declared or specified.
        p++;
    }

    // Equivalent to !$omp parallel default(private) shared(a, b, c):
    // default(private) means that any variable not explicitly listed as shared is private.
    // We must ensure a, b, c are known and shared:
    a = 1; b = 2; c = 3;
    #pragma omp parallel default(private) shared(a, b, c)
    {
        // Here a, b, c are shared. Any other variable is private by default.
        a++; // affects the shared variable
    }

    // Equivalent to !$omp parallel firstprivate(i):
    // This means i is private, but initialized from the value outside the parallel region.
    int i = 42;
    #pragma omp parallel firstprivate(i)
    {
        // Each thread gets its own copy of i, initialized to 42
        i += omp_get_thread_num();
    }

    return 0;
}
```

## OpenMP cpp 语法

编译: `g++ -fopenmp`

[其它编译器](https://hpc-tutorials.llnl.gov/openmp/compiling/):

| Compiler / Platform      | Compiler                                    | Flag         |
|---------------------------|---------------------------------------------|--------------|
| Intel Linux Opteron/Xeon | icc, icpc, ifort                            | -qopenmp     |
| PGI Linux Opteron/Xeon   | pgcc, pgCC, pgf77, pgf90                    | -mp          |
| GNU Linux Opteron/Xeon   | gcc, g++, g77, gfortran                     | -fopenmp     |


在 C 和 C++ 中，所有 OpenMP 指令都通过 `#pragma omp` 指定，并带有参数，以换行符结束。  
通常，`#pragma omp` 只适用于其后紧跟的一条语句，  但 `barrier` 和 `flush` 指令是例外，它们没有关联的语句。


### `parallel`

`parallel` 用于启动一个并行块。**它会创建一个包含 N 个 thread 的 *team***（N 的大小在运行时确定，通常由 CPU 核心数量决定，但也可能受其他因素影响）。这些线程会同时执行下一条语句（或者如果下一条语句是一个用 `{...}` 包裹的代码块，则执行整个代码块）。在语句执行完成后，所有线程会合并为一个线程。

语法: `#pragma omp parallel`

举例，

```cpp
  #pragma omp parallel
  {
    // Code inside this region runs in parallel.
    printf("Hello!\n");
  }
```

#### Parallelism conditionality clause `if`

举例
```cpp
  extern int parallelism_enabled;
  #pragma omp parallel for if(parallelism_enabled)
  for(int c=0; c<n; ++c)
    handle(c);
```
如果 `parallelism_enabled` 的值\为零，则处理 `for` 循环的线程team的线程数量将始终为 1。


### `for` loop

语法: `#pragma omp for` (只在当前的 parallel team 中在不同线程上分配任务，如果想在新的 team 上分配任务，就用 `#pragma omp parallel` 创建新的 block)

举例: 

```cpp
 #pragma omp for
 for(int n=0; n<10; ++n)
 {
   printf(" %d", n);
 }
 printf(".\n");
```

创建新的 thread team:

```cpp
 #pragma omp parallel
 {
  #pragma omp for
  for(int n=0; n<10; ++n) printf(" %d", n);
 }
 printf(".\n");
```
等价于
```cpp
 #pragma omp parallel for
 for(int n=0; n<10; ++n) printf(" %d", n);
 printf(".\n")
```


也可以直接指定线程数

```cpp
 #pragma omp parallel num_threads(3)
 {
   // This code will be executed by three threads.
   
   // Chunks of this loop will be divided amongst
   // the (three) threads of the current team.
   #pragma omp for
   for(int n=0; n<10; ++n) printf(" %d", n);
 }
```
或者 `#pragma omp parallel for num_threads(3)`


#### `schedule`

for 循环的调度分配算法是可以被显式控制的

```cpp
 #pragma omp for schedule(static)
 for(int n=0; n<10; ++n) printf(" %d", n);
 printf(".\n");
```

OpenMP 提供了五种调度类型 (scheduling types)：`static`、`dynamic`、`guided`、`auto` 和（从 OpenMP 4.0 开始支持的）`runtime`。   此外，从 OpenMP 4.5 开始，新增了三种调度修饰符 (scheduling modifiers)：`monotonic`、`nonmonotonic` 和 `simd`。

`static` 是默认的调度类型。在进入循环时，每个线程会独立决定自己将处理的循环块。

此外，还有 `dynamic` 调度类型。循环开始时并不会提前固定分配，而是线程在运行时向 OpenMP 运行时库动态请求任务。每个线程会向 OpenMP 运行时库请求一个迭代号，处理完后再请求下一个，如此往复。  这种方式可以避免线程因分配的工作完成而闲置。例如，对于一个 `for` 循环有 10 次迭代，假设有 2 个线程：线程 1 请求并处理迭代 0, 线程 2 请求并处理迭代 1,  线程 1 完成后继续请求迭代 2，依此类推。


当与 `ordered` 子句一起使用，或者循环中的不同迭代可能需要不同的执行时间时，这种调度方式最为有用。

还可以指定块大小（chunk size）以减少对运行时库的调用次数：
```cpp
 #pragma omp for schedule(dynamic, 3)
 for(int n=0; n<10; ++n) printf(" %d", n);
 printf(".\n");
```
这个时候每个线程每次动态地索要三个 iteration number。

还有其它不常用的 scheduling type:
* `guided`: `#pragma omp parallel for schedule(guided, 2)`。动态分配任务块，但任务块的大小会随着循环的进展逐渐减小，起初，任务块的大小较大（通常与循环迭代总数相关），然后逐步减小到指定的最小块大小，这里是 2。适用于: 循环的迭代耗时不均，但倾向于后续迭代任务较少或较轻。
* `auto`: 由OpenMP自动决定调度策略 `#pragma omp parallel for schedule(auto)`
* `runtime`: `#pragma omp parallel for schedule(runtime)`。用户可以通过环境变量或 API 配置。

scheduling modifiers：
*`monotonic` , `#pragma omp for schedule(monotonic:static, 4)`, 确保线程执行迭代的顺序与原始循环的顺序一致
* `nonmonotonic`, 使用 `nonmonotonic` 修饰符时，线程以未指定的顺序执行分配给它们的块
* `simd`，





## 例1, `omp_set_num_threads(num)` 

* 对应 fortran `call omp_set_num_threads()`

课程中的 fortran code: 

```fortran
program main
  use omp_lib
  implicit none
  integer :: omp_i

  call omp_set_num_threads(8)

  !$omp parallel
    omp_i = omp_get_thread_num()
    if (omp_i == 0) write(*,*) "0"
  !$omp end parallel

end program main
```

```bash
gfortran -O3 -fdefault-real-8 -fopenmp pi.f90 && ./a.out
```

对应的 cpp code:

```cpp
#include <iostream>
#include <omp.h>

int main() {
    omp_set_num_threads(8);

    #pragma omp parallel
    {
        int omp_i = omp_get_thread_num();
        if (omp_i == 0) {
            std::cout << "0\n";
        }
    } 

    return 0;
}
```

```bash
 g++ -O3 -fopenmp main.cpp  && ./a.out
```




## 一个简单的例子: 求 $\pi$


![输入图片说明](https://github.com/ymma98/picx-images-hosting/raw/master/20241210/image.8ojqx1zajr.webp){width="600"}

$\pi$ 可以用 $\frac{4}{1+x^2}$ 函数在 0 到 1 范围内的积分求解实现。推导: $x=\tan\theta$。




* fortran 程序

pi.f90



```fortran
program main
  implicit none
  integer(8) :: n, i
  real :: pi, dx, x
  real, parameter :: pi0 = &
  3.14159265358979323846264338327950288419716939937510582097

  n = 10000000000_8
  dx = 1.0 / real(n)

  pi = 0.0
  do i = 0, n
     x = dx * i
     pi = pi + 4.0 * dx / (1.0 + x * x)
  end do

  write(*,*) pi, abs(pi - pi0) / pi0
end program main
```

```bash
# 在 Fortran 中，`REAL` 类型的默认大小是 4 字节（32 位），使用 `-fdefault-real-8` 将所有 `REAL` 和 `COMPLEX` 类型自动提升为双精度（64 位）。
gfortran -fdefault-real-8 pi.f90
```

* cpp 程序

```cpp
#include <iostream>
#include <cmath>
#include <cstdint>

// We choose a constexpr double for pi0 for clarity and precision.
// This is a known approximation of π with many digits.
constexpr double pi0 = 3.14159265358979323846264338327950288419716939937510582097;

// This program approximates π by integrating the function f(x) = 4/(1+x²) from x = 0 to x = 1.
// The integral of f(x) from 0 to 1 is π.
int main() {
    // Use a 64-bit integer type for n, given the large range.
    // In Fortran, `integer(8)` typically corresponds to a 64-bit integer.
    // The original Fortran code used n = 10,000,000,000 (ten billion) as a demonstration.
    std::int64_t n = 10000000000LL; 

    // dx = 1.0 / n
    double dx = 1.0 / static_cast<double>(n);

    // Initialize pi accumulator
    double pi = 0.0;

    // Perform the summation (Riemann sum)
    // Note: This is a very large loop and would be time-consuming in practice.
    for (std::int64_t i = 0; i <= n; ++i) {
        double x = dx * static_cast<double>(i);
        // Increment pi by the area of the small rectangle: f(x)*dx
        pi += (4.0 * dx) / (1.0 + x * x);
    }

    // Compute the relative error
    double relative_error = std::fabs(pi - pi0) / pi0;

    // Print results
    // The first output is the computed approximation of pi.
    // The second output is the relative error compared to pi0.
    std::cout << pi << " " << relative_error << "\n";

    return 0;
}
```
<!--stackedit_data:
eyJoaXN0b3J5IjpbOTIxNTQ5MTgyLC01NTQ2MTI0ODMsLTUwND
YzMDIzOSwtMTE5MDk3ODQxNSwtNjc4MzQ4MDY0LC05MTU5NDg3
MTMsLTE3ODg5NDQ3ODYsLTY4ODQ1OTY2MSw0NDg5OTcwMTEsLT
EwMzI3NTQ4MDQsLTEwMDg2NzYyMjcsLTExNzY4NDQ1NzEsLTQ1
Njc1Mjk4MCwxNzQ1NjYwMzIxLDE2NTA0OTM3MTUsLTIxMjkzOD
E4NDIsLTk2NzQxMDU5MiwtMzE5NDIyNTYyLC0xODU4MjM0NDc2
XX0=
-->