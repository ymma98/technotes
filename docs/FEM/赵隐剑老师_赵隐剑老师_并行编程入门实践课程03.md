# 赵隐剑老师_并行编程入门实践课程 03

* 该笔记对应以下 b 站视频，主要讲在粒子-粒子方法中应用 openmp + mpi
	*  [并行编程入门与实践5](https://www.bilibili.com/video/BV1xHz7YFEMy/?spm_id_from=333.1387.homepage.video_card.click&vd_source=b7bbd99721bfe117cc47d14c9f45af86)


[TOC]

___

## 粒子-粒子方法

### 推导以及无量纲化

* 库仑定律 (Coulomb's Law)

$$
\vec{F}_{12} = \frac{1}{4 \pi \epsilon_0} \frac{q_1 q_2}{|\vec{r}_{12}|^2} \hat{r}_{12}
$$

其中：
$$
\vec{r}_{12} = \vec{r}_1 - \vec{r}_2, \quad \hat{r}_{12} = \frac{\vec{r}_{12}}{|\vec{r}_{12}|}
$$

$$
\vec{r}_1 = (x_1, y_1, z_1), \quad \vec{r}_2 = (x_2, y_2, z_2)
$$


* 电场与力的关系

$$
F = \vec{E} q \quad \Rightarrow \quad \vec{E}_{12} = \frac{1}{4 \pi \epsilon_0} \frac{q_2}{|\vec{r}_{12}|^2} \hat{r}_{12}
$$



* 多粒子系统 (Many Particles)

对于粒子 $j$ 对粒子$i$的作用：
$$
\vec{E}_{ij} = \frac{1}{4 \pi \epsilon_0} \frac{q_j}{|\vec{r}_{ij}|^2} \hat{r}_{ij}
$$

总场为：
$$
\vec{E}_i = \sum_{\substack{j=1 \\ j \neq i}}^N \vec{E}_{ij} = \frac{1}{4 \pi \epsilon_0} \sum_{\substack{j=1 \\ j \neq i}}^N \frac{q_j}{|\vec{r}_{ij}|^2} \hat{r}_{ij}
$$

*  宏粒子

对于一个模拟粒子系统，假定体积 $V$，粒子密度为$n$

设权重：

$$
w = \frac{nV}{N}
$$

则：
$$
\vec{E}_i = \frac{1}{4 \pi \epsilon_0} \sum_{\substack{j=1 \\ j \neq i}}^N \frac{w q_j}{|\vec{r}_{ij}|^2} \hat{r}_{ij}
$$



令 $q_j = q$，并定义：
$$
k_e = \frac{w q}{4 \pi \epsilon_0}
$$

因此：
$$
\vec{E}_i = k_e \sum_{\substack{j=1 \\ j \neq i}}^N \frac{\hat{r}_{ij}}{|\vec{r}_{ij}|^2}
$$



伪代码如下：

do $i = 1, N$  
&emsp; do $j = 1, N$  
&emsp;&emsp; 更新电场：$\vec{E}_i \;=\; \vec{E}_i \;+\; k_e \,\dfrac{\hat{r}_{ij}}{\lvert\vec{r}_{ij}\rvert^2}$  
&emsp; end do  
end do



$$
\vec{v}_i^{\,n+1} 
= \vec{v}_i^{\,n} + \Delta t \cdot \vec{a}_i
\quad,\quad
\vec{a}_i 
= \frac{\vec{E}_i \, e w}{m w}
$$

$$
\vec{x}_i^{\,n+1} 
= \vec{x}_i^{\,n} + \Delta t \cdot \vec{v}_i^{\,n+1}
$$

P.S.  

$$
\frac{d\vec{x}}{dt} = \vec{v}, 
\quad 
\frac{d\vec{v}}{dt} = \vec{a}
$$


* 无量纲化 (Normalization)

1. 使数量级 $\sim 1$，避免过大或过小的数。  
2. 使一些量 $=1$，从计算中省去。

SI Base Units:  
- Mass = kg  
- Charge = C  
- Time = s  
- Length = m



$M' = 1,\quad q' = 1,\quad \Delta t' = 1,\quad k_e' = 1$


* 长度无量纲化推导

$k_e = \frac{w q}{4 \pi \varepsilon_0}$

$k_e' = \frac{w q'}{4 \pi \varepsilon_0} = \frac{w \frac{q}{\varepsilon_0}}{4 \pi \varepsilon_0 / (C^2 T^2 M^{-1} L^{-3})}$

因此，

$$
1 = k_e' = \frac{w}{4 \pi \varepsilon_0} C^2 T^2 M^{-1} L^{-3}
$$

令：

$$
L^3 = \frac{w C^2 T^2 M^{-1}}{4 \pi \varepsilon_0}
$$

解得：

$$
L = \sqrt[3]{\frac{w C^2 T^2}{4 \pi \varepsilon_0 M}}
$$



* python 代码

```python
import numpy as np
import scipy.constants as sc

ep0 = sc.epsilon_0
pi  = sc.pi
e   = sc.e

print("ep0, pi, e:")
print(ep0, pi, e)

dt = 1.5e-11   # s
n0 = 5.0e16    # m^-3
dd = 0.1e-3    # m

# Mass in kg
me = sc.m_e

# Domain size
LL = 64 * dd

# Domain volume
VV = LL**3

# Number of particles
N = 50000

# Particle weight
w0 = VV * n0 / N
print("w0:", w0)

# Charge
qe = -e

# 这里把电子温度设为 10 eV，
# 若要表达成“10 eV 对应多少 J”，
# 则 1 eV = sc.e (J)，
# 所以 Te = 10 eV => Te (in J) = 10 * sc.e
Te = 10.0 * sc.e   # 10 eV in Joules

# 热速度 (thermal velocity)
vte = np.sqrt(Te * sc.Boltzmann / me)  # m/s
# 等离子体频率
wpe = np.sqrt(n0 * sc.e**2 / me / sc.epsilon_0)

# 可能是德拜长度 (lambda_D)
lmd = np.sqrt(sc.epsilon_0 * Te * sc.Boltzmann / n0 / sc.e / sc.e)

# 正进行“无量纲化”的定义 (Normalization)
# Charge
C = e
# Mass
M = me
# Time
T = dt
# Length
#(如果想直接取 dd 为长度单位，可改)
#L = dd
# 这里根据截图似乎用了某种立方根关系
L = np.cbrt(w0 * C**2 * T**2 / (4 * pi * ep0 * M))
# Velocity
V = L / T

print("C, M, T, L, V")
print(C, M, T, L, V)

# 归一化后的 eps0
ep0_ = ep0 / (C**2 * T * M / (L**3))
print("ep0_ =", ep0_)
#   A^2·s^4·kg^-1·m^-3
# = C^2·s^2·kg^-2·m^-3

# 归一化后的电荷
e_ = e / C
print("e_ =", e_)

# 归一化后的 k_e
ke_ = w0 * e_ / (4 * pi * ep0_)
print("ke_ =", ke_)

print("vte =", vte / V)
print("n0  =", n0 * (L**3))
print("lmd =", lmd / L)
print("LL  =", LL / L)
```

### fortran code 

* 子程序

```fortran
subroutine sub_ppl(n, p1, p2, p, c, mpi_n)
  use mpi
  use omp_lib

  implicit none

  integer          :: n, p1, p2
  real, dimension(1:9, 1:n) :: p  ! 9 x n array
  real             :: c
  integer          :: mpi_n
  integer          :: i, j
  real             :: r(1:3), rm

  ! Begin OpenMP parallel region
  !$omp parallel default(firstprivate) shared(p)
  !$omp do
  do i = p1, p2
     do j = 1, n
        if (j == i) cycle

        ! Compute the 3D difference vector r = p(1:3,i) - p(1:3,j)
        r(1:3) = p(1:3, i) - p(1:3, j)

        ! Calculate the distance rm = sqrt(r(1)^2 + r(2)^2 + r(3)^2)
        rm = sqrt(r(1)*r(1) + r(2)*r(2) + r(3)*r(3))

        ! If distance <= c, skip
        if (rm <= c) cycle

        ! Update p(7:9, i) by adding r/rm^3
        p(7:9, i) = p(7:9, i) + r(1:3) / (rm*rm*rm)
     end do

     ! Update p(4:6,i) by adding p(7:9,i)
     p(4:6, i) = p(4:6, i) + p(7:9, i)

     ! Update p(1:3,i) by adding p(4:6,i)
     p(1:3, i) = p(1:3, i) + p(4:6, i)
  end do
  !$omp end do

  ! If running in MPI with more than 1 process, zero out the edges
  if (mpi_n > 1) then
    !$omp workshare
    p(1:9, 1:p1-1) = 0.0
    p(1:9, p2+1:n) = 0.0
    !$omp end workshare
  end if

  !$omp end parallel
end subroutine sub_ppl
```

* 主程序

### cpp code


```cpp
#include <iostream>
#include <vector>
#include <array>
#include <cmath>
#ifdef _OPENMP
  #include <omp.h>
#endif

/**
 * @brief Modern C++ version of the Fortran subroutine sub_ppl.
 *
 * @param n      The size of the second dimension of p (number of columns).
 * @param p1     Starting index (1-based in Fortran).
 * @param p2     Ending index (1-based in Fortran).
 * @param p      9 x n data array (in C++, stored as a vector of 9-element arrays).
 * @param c      The cutoff distance.
 * @param mpi_n  The number of MPI processes (used to conditionally reset certain data).
 */
void sub_ppl(int n,
             int p1,
             int p2,
             std::vector<std::array<double, 9>>& p,
             double c,
             int mpi_n)
{
    // Fortran uses 1-based indexing. In C++, it's 0-based. So we map:
    //   Fortran i in [p1, p2]  -> C++ i in [p1-1, p2-1]
    //   Fortran j in [1, n]    -> C++ j in [0, n-1]
    //
    // p[k][0..8] in C++ corresponds to p(1..9,k+1) in Fortran (just reversed dimension order).
    // So p[i][0] = p(1, i+1), p[i][1] = p(2, i+1), ... p[i][8] = p(9, i+1).

    // OpenMP parallel region (equivalent to !$omp parallel ...)
    #pragma omp parallel default(none) \
                         shared(p, n, p1, p2, c, mpi_n) \
                         firstprivate(/* no large copies, just scalar captures here */)
    {
        // Parallel for (equivalent to !$omp do in Fortran)
        #pragma omp for
        for (int i = p1 - 1; i < p2; ++i)
        {
            // For each i, loop over j from 0 to n-1
            for (int j = 0; j < n; ++j)
            {
                // Skip if j == i (matching "if (j == i) cycle")
                if (j == i) 
                    continue;

                // r(1:3) = p(1:3, i) - p(1:3, j)
                // We'll store this difference in a small local array r[0..2].
                std::array<double, 3> r {
                    p[i][0] - p[j][0],  // x-component
                    p[i][1] - p[j][1],  // y-component
                    p[i][2] - p[j][2]   // z-component
                };

                // rm = sqrt(r(1)^2 + r(2)^2 + r(3)^2)
                double rm = std::sqrt(r[0]*r[0] + r[1]*r[1] + r[2]*r[2]);

                // if (rm <= c) cycle
                if (rm <= c)
                    continue;

                // p(7:9, i) = p(7:9, i) + r(1:3) / (rm^3)
                // In C++: p[i][6..8] += r[0..2]/(rm^3)
                double inv_rm3 = 1.0 / (rm * rm * rm);
                p[i][6] += r[0] * inv_rm3;
                p[i][7] += r[1] * inv_rm3;
                p[i][8] += r[2] * inv_rm3;
            }

            // p(4:6, i) = p(4:6, i) + p(7:9, i)
            // So p[i][3..5] += p[i][6..8]
            p[i][3] += p[i][6];
            p[i][4] += p[i][7];
            p[i][5] += p[i][8];

            // p(1:3, i) = p(1:3, i) + p(4:6, i)
            // So p[i][0..2] += p[i][3..5]
            p[i][0] += p[i][3];
            p[i][1] += p[i][4];
            p[i][2] += p[i][5];
        }

        // if (mpi_n > 1) then
        //   p(1:9, 1:p1-1) = 0.0
        //   p(1:9, p2+1:n) = 0.0
        // end if
        if (mpi_n > 1)
        {
            // We can do two separate parallel loops if desired:
            #pragma omp for
            for (int idx = 0; idx < p1 - 1; ++idx)
            {
                for (int row = 0; row < 9; ++row)
                {
                    p[idx][row] = 0.0;
                }
            }

            #pragma omp for
            for (int idx = p2; idx < n; ++idx)
            {
                for (int row = 0; row < 9; ++row)
                {
                    p[idx][row] = 0.0;
                }
            }
        }
    } // end of parallel region
}
```


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTYwODY5NTE5Nyw0NjM1ODQzNiwzNTI2Nj
Y0OTksLTU2NzQ1MzUxNCw1NTM0NDE2MzYsLTE4MDI2Njk4OTNd
fQ==
-->